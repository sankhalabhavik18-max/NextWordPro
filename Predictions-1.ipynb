{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Prediction Script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load the model and tokenizer\n",
    "\n",
    "model = load_model('nextword1.h5')\n",
    "tokenizer = pickle.load(open('tokenizer1.pkl', 'rb'))\n",
    "\n",
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "    \"\"\"\n",
    "        In this function we are using the tokenizer and models trained\n",
    "        and we are creating the sequence of the text entered and then\n",
    "        using our model to predict and return the the predicted word.\n",
    "    \n",
    "    \"\"\"\n",
    "    # for i in range(3):\n",
    "    print(text)\n",
    "    print(model)\n",
    "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "    sequence = np.array(sequence)\n",
    "    print(sequence)\n",
    "    preds = model.predict(sequence)\n",
    "    print(preds)\n",
    "    # Convert the prediction list to a numpy array\n",
    "    preds_array = np.array(preds)\n",
    "\n",
    "    # Get the indices of the maximum values\n",
    "    max_indices = np.argsort(preds_array)[0, ::-1]\n",
    "\n",
    "    # # Extract the second and third maximum indices\n",
    "    # second_max_index = max_indices[1]\n",
    "    # third_max_index = max_indices[2]\n",
    "    preds = max_indices[0]\n",
    "    # print(\"Second maximum index:\", second_max_index)\n",
    "    # print(\"Third maximum index:\", third_max_index)\n",
    "    # print('preds: ',len(preds[0]))\n",
    "    predicted_word = \"\"\n",
    "    # print(len(tokenizer.word_index.items()))\n",
    "    for key, value in tokenizer.word_index.items():\n",
    "        # print('val: ',value,key)\n",
    "        # print(int(value))\n",
    "        # print(int(preds))\n",
    "        if int(value) == int(preds):\n",
    "            print(key)\n",
    "            predicted_word = key\n",
    "            print(predicted_word)\n",
    "            break\n",
    "    \n",
    "    print(predicted_word)\n",
    "    return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dull\n",
      "dull\n",
      "<keras.engine.sequential.Sequential object at 0x000001EA000CB520>\n",
      "[785]\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EA05BBADC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "[[4.5096551e-33 3.0477256e-30 1.7714369e-34 ... 0.0000000e+00\n",
      "  1.0049185e-22 2.7164801e-37]]\n",
      "weather\n",
      "weather\n",
      "weather\n",
      "prrr:  weather\n",
      "textile\n",
      "textile\n",
      "<keras.engine.sequential.Sequential object at 0x000001EA000CB520>\n",
      "[762]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[[1.3554668e-33 3.3319720e-13 5.5429217e-16 ... 4.2814276e-22\n",
      "  4.4045525e-27 0.0000000e+00]]\n",
      "samples\n",
      "samples\n",
      "samples\n",
      "prrr:  samples\n",
      "strenuous\n",
      "strenuous\n",
      "<keras.engine.sequential.Sequential object at 0x000001EA000CB520>\n",
      "[809]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "[[1.8132207e-31 2.4442113e-32 0.0000000e+00 ... 6.4598334e-11\n",
      "  0.0000000e+00 1.5240658e-27]]\n",
      "career\n",
      "career\n",
      "career\n",
      "prrr:  career\n",
      "hello\n",
      "hello\n",
      "<keras.engine.sequential.Sequential object at 0x000001EA000CB520>\n",
      "[]\n",
      "hey\n",
      "hey\n",
      "<keras.engine.sequential.Sequential object at 0x000001EA000CB520>\n",
      "[]\n",
      "great\n",
      "great\n",
      "<keras.engine.sequential.Sequential object at 0x000001EA000CB520>\n",
      "[1283]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "surprising\n",
      "surprising\n",
      "surprising\n",
      "prrr:  surprising\n",
      "bhavik\n",
      "bhavik\n",
      "<keras.engine.sequential.Sequential object at 0x000001EA000CB520>\n",
      "[]\n",
      "so\n",
      "so\n",
      "<keras.engine.sequential.Sequential object at 0x000001EA000CB520>\n",
      "[12]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[[4.1915190e-33 1.4113274e-09 3.5935098e-13 ... 0.0000000e+00\n",
      "  5.5572451e-34 2.7067282e-22]]\n",
      "shown\n",
      "shown\n",
      "shown\n",
      "prrr:  shown\n",
      "om\n",
      "om\n",
      "<keras.engine.sequential.Sequential object at 0x000001EA000CB520>\n",
      "[]\n",
      "om\n",
      "om\n",
      "<keras.engine.sequential.Sequential object at 0x000001EA000CB520>\n",
      "[]\n",
      "namah\n",
      "namah\n",
      "<keras.engine.sequential.Sequential object at 0x000001EA000CB520>\n",
      "[]\n",
      "lady\n",
      "lady\n",
      "<keras.engine.sequential.Sequential object at 0x000001EA000CB520>\n",
      "[314]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[[3.738203e-26 0.000000e+00 0.000000e+00 ... 7.304098e-33 0.000000e+00\n",
      "  0.000000e+00]]\n",
      "quieter\n",
      "quieter\n",
      "quieter\n",
      "prrr:  quieter\n",
      "Ending The Program.....\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    We are testing our model and we will run the model\n",
    "    until the user decides to stop the script.\n",
    "    While the script is running we try and check if \n",
    "    the prediction can be made on the text. If no\n",
    "    prediction can be made we just continue.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# text1 = \"at the dull\"\n",
    "# text2 = \"collection of textile\"\n",
    "# text3 = \"what a strenuous\"\n",
    "# text4 = \"stop the script\"\n",
    "\n",
    "while(True):\n",
    "\n",
    "    text = input(\"Enter your line: \")\n",
    "    \n",
    "    if text == \"stop the script\":\n",
    "        print(\"Ending The Program.....\")\n",
    "        break\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            text = text.split(\" \")\n",
    "            text = text[-1]\n",
    "\n",
    "            text = ''.join(text)\n",
    "            print(text)\n",
    "            prr=Predict_Next_Words(model, tokenizer, text)\n",
    "            print(\"prrr: \",prr)\n",
    "            \n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
