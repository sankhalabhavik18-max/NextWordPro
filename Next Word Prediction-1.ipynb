{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Word Prediction:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing The Required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Line:  ï»¿One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "\n",
      "The Last Line:  first to get up and stretch out her young body.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Dataset: http://www.gutenberg.org/cache/epub/5200/pg5200.txt\n",
    "    Remove all the unnecessary data and label it as Metamorphosis-clean.\n",
    "    The starting and ending lines should be as follows.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "file = open(\"metamorphosis_clean.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "    \n",
    "print(\"The First Line: \", lines[0])\n",
    "print(\"The Last Line: \", lines[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "data[:360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning  when Gregor Samsa woke from troubled dreams  he found himself transformed in his bed into a horrible vermin   He lay on his armour like back  and if he lifted his head a little he could see his brown belly  slightly domed and divided by arches into stiff sections   The bedding was hardly able to cover it and seemed ready to slide off any moment   His many legs  pitifully thin compared with the size of the rest of him  waved about helplessly as he looked    What s happened to me   he'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)\n",
    "\n",
    "new_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided by arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. \"What\\'s happened me?\" thought. It wasn\\'t dream. room, proper human room altho'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in z:\n",
    "        z.append(i)\n",
    "        \n",
    "data = ' '.join(z)\n",
    "data[:500]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 53, 293, 2, 18, 729, 135, 730, 294, 8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2617\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  3889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 17,  53],\n",
       "       [ 53, 293],\n",
       "       [293,   2],\n",
       "       [  2,  18],\n",
       "       [ 18, 729],\n",
       "       [729, 135],\n",
       "       [135, 730],\n",
       "       [730, 294],\n",
       "       [294,   8],\n",
       "       [  8, 731]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [ 17  53 293   2  18]\n",
      "The responses are:  [ 53 293   2  18 729]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Data is: \", X[:5])\n",
    "print(\"The responses are: \", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1, 10)             26170     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2617)              2619617   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,694,787\n",
      "Trainable params: 15,694,787\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True,mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shubhangi\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.8754\n",
      "Epoch 1: loss improved from inf to 7.87543, saving model to nextword1.h5\n",
      "61/61 [==============================] - 26s 298ms/step - loss: 7.8754 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.8626\n",
      "Epoch 2: loss improved from 7.87543 to 7.86264, saving model to nextword1.h5\n",
      "61/61 [==============================] - 19s 309ms/step - loss: 7.8626 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.8281\n",
      "Epoch 3: loss improved from 7.86264 to 7.82807, saving model to nextword1.h5\n",
      "61/61 [==============================] - 19s 306ms/step - loss: 7.8281 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.7361\n",
      "Epoch 4: loss improved from 7.82807 to 7.73614, saving model to nextword1.h5\n",
      "61/61 [==============================] - 21s 344ms/step - loss: 7.7361 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.5359\n",
      "Epoch 5: loss improved from 7.73614 to 7.53591, saving model to nextword1.h5\n",
      "61/61 [==============================] - 23s 374ms/step - loss: 7.5359 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.2799\n",
      "Epoch 6: loss improved from 7.53591 to 7.27985, saving model to nextword1.h5\n",
      "61/61 [==============================] - 21s 350ms/step - loss: 7.2799 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.9698\n",
      "Epoch 7: loss improved from 7.27985 to 6.96976, saving model to nextword1.h5\n",
      "61/61 [==============================] - 20s 334ms/step - loss: 6.9698 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.6848\n",
      "Epoch 8: loss improved from 6.96976 to 6.68481, saving model to nextword1.h5\n",
      "61/61 [==============================] - 22s 366ms/step - loss: 6.6848 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.4314\n",
      "Epoch 9: loss improved from 6.68481 to 6.43140, saving model to nextword1.h5\n",
      "61/61 [==============================] - 21s 339ms/step - loss: 6.4314 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.2046\n",
      "Epoch 10: loss improved from 6.43140 to 6.20461, saving model to nextword1.h5\n",
      "61/61 [==============================] - 22s 365ms/step - loss: 6.2046 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.0093\n",
      "Epoch 11: loss improved from 6.20461 to 6.00932, saving model to nextword1.h5\n",
      "61/61 [==============================] - 22s 366ms/step - loss: 6.0093 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.8229\n",
      "Epoch 12: loss improved from 6.00932 to 5.82290, saving model to nextword1.h5\n",
      "61/61 [==============================] - 23s 381ms/step - loss: 5.8229 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.6505\n",
      "Epoch 13: loss improved from 5.82290 to 5.65049, saving model to nextword1.h5\n",
      "61/61 [==============================] - 22s 368ms/step - loss: 5.6505 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.5022\n",
      "Epoch 14: loss improved from 5.65049 to 5.50217, saving model to nextword1.h5\n",
      "61/61 [==============================] - 22s 356ms/step - loss: 5.5022 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.3502\n",
      "Epoch 15: loss improved from 5.50217 to 5.35020, saving model to nextword1.h5\n",
      "61/61 [==============================] - 23s 375ms/step - loss: 5.3502 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.2070\n",
      "Epoch 16: loss improved from 5.35020 to 5.20704, saving model to nextword1.h5\n",
      "61/61 [==============================] - 24s 401ms/step - loss: 5.2070 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.0649\n",
      "Epoch 17: loss improved from 5.20704 to 5.06491, saving model to nextword1.h5\n",
      "61/61 [==============================] - 25s 407ms/step - loss: 5.0649 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.8766\n",
      "Epoch 18: loss improved from 5.06491 to 4.87658, saving model to nextword1.h5\n",
      "61/61 [==============================] - 29s 479ms/step - loss: 4.8766 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.6872\n",
      "Epoch 19: loss improved from 4.87658 to 4.68718, saving model to nextword1.h5\n",
      "61/61 [==============================] - 25s 405ms/step - loss: 4.6872 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.4967\n",
      "Epoch 20: loss improved from 4.68718 to 4.49672, saving model to nextword1.h5\n",
      "61/61 [==============================] - 28s 459ms/step - loss: 4.4967 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.2883\n",
      "Epoch 21: loss improved from 4.49672 to 4.28833, saving model to nextword1.h5\n",
      "61/61 [==============================] - 29s 481ms/step - loss: 4.2883 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.1181\n",
      "Epoch 22: loss improved from 4.28833 to 4.11812, saving model to nextword1.h5\n",
      "61/61 [==============================] - 28s 458ms/step - loss: 4.1181 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.9634\n",
      "Epoch 23: loss improved from 4.11812 to 3.96340, saving model to nextword1.h5\n",
      "61/61 [==============================] - 28s 456ms/step - loss: 3.9634 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.8033\n",
      "Epoch 24: loss improved from 3.96340 to 3.80333, saving model to nextword1.h5\n",
      "61/61 [==============================] - 28s 466ms/step - loss: 3.8033 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.6220\n",
      "Epoch 25: loss improved from 3.80333 to 3.62199, saving model to nextword1.h5\n",
      "61/61 [==============================] - 31s 506ms/step - loss: 3.6220 - lr: 0.0010\n",
      "Epoch 26/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.4837\n",
      "Epoch 26: loss improved from 3.62199 to 3.48374, saving model to nextword1.h5\n",
      "61/61 [==============================] - 27s 437ms/step - loss: 3.4837 - lr: 0.0010\n",
      "Epoch 27/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.3551\n",
      "Epoch 27: loss improved from 3.48374 to 3.35508, saving model to nextword1.h5\n",
      "61/61 [==============================] - 27s 447ms/step - loss: 3.3551 - lr: 0.0010\n",
      "Epoch 28/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.2629\n",
      "Epoch 28: loss improved from 3.35508 to 3.26287, saving model to nextword1.h5\n",
      "61/61 [==============================] - 28s 462ms/step - loss: 3.2629 - lr: 0.0010\n",
      "Epoch 29/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.1548\n",
      "Epoch 29: loss improved from 3.26287 to 3.15478, saving model to nextword1.h5\n",
      "61/61 [==============================] - 31s 508ms/step - loss: 3.1548 - lr: 0.0010\n",
      "Epoch 30/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.0758\n",
      "Epoch 30: loss improved from 3.15478 to 3.07577, saving model to nextword1.h5\n",
      "61/61 [==============================] - 25s 402ms/step - loss: 3.0758 - lr: 0.0010\n",
      "Epoch 31/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.0136\n",
      "Epoch 31: loss improved from 3.07577 to 3.01362, saving model to nextword1.h5\n",
      "61/61 [==============================] - 27s 436ms/step - loss: 3.0136 - lr: 0.0010\n",
      "Epoch 32/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.8898\n",
      "Epoch 32: loss improved from 3.01362 to 2.88982, saving model to nextword1.h5\n",
      "61/61 [==============================] - 27s 453ms/step - loss: 2.8898 - lr: 0.0010\n",
      "Epoch 33/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.7982\n",
      "Epoch 33: loss improved from 2.88982 to 2.79823, saving model to nextword1.h5\n",
      "61/61 [==============================] - 27s 449ms/step - loss: 2.7982 - lr: 0.0010\n",
      "Epoch 34/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.7510\n",
      "Epoch 34: loss improved from 2.79823 to 2.75102, saving model to nextword1.h5\n",
      "61/61 [==============================] - 29s 466ms/step - loss: 2.7510 - lr: 0.0010\n",
      "Epoch 35/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.6848\n",
      "Epoch 35: loss improved from 2.75102 to 2.68479, saving model to nextword1.h5\n",
      "61/61 [==============================] - 26s 425ms/step - loss: 2.6848 - lr: 0.0010\n",
      "Epoch 36/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.6226\n",
      "Epoch 36: loss improved from 2.68479 to 2.62261, saving model to nextword1.h5\n",
      "61/61 [==============================] - 26s 422ms/step - loss: 2.6226 - lr: 0.0010\n",
      "Epoch 37/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.5790\n",
      "Epoch 37: loss improved from 2.62261 to 2.57903, saving model to nextword1.h5\n",
      "61/61 [==============================] - 26s 422ms/step - loss: 2.5790 - lr: 0.0010\n",
      "Epoch 38/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.5071\n",
      "Epoch 38: loss improved from 2.57903 to 2.50711, saving model to nextword1.h5\n",
      "61/61 [==============================] - 24s 394ms/step - loss: 2.5071 - lr: 0.0010\n",
      "Epoch 39/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.4493\n",
      "Epoch 39: loss improved from 2.50711 to 2.44927, saving model to nextword1.h5\n",
      "61/61 [==============================] - 25s 420ms/step - loss: 2.4493 - lr: 0.0010\n",
      "Epoch 40/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.3889\n",
      "Epoch 40: loss improved from 2.44927 to 2.38888, saving model to nextword1.h5\n",
      "61/61 [==============================] - 26s 425ms/step - loss: 2.3889 - lr: 0.0010\n",
      "Epoch 41/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.3728\n",
      "Epoch 41: loss improved from 2.38888 to 2.37283, saving model to nextword1.h5\n",
      "61/61 [==============================] - 26s 417ms/step - loss: 2.3728 - lr: 0.0010\n",
      "Epoch 42/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.3120\n",
      "Epoch 42: loss improved from 2.37283 to 2.31203, saving model to nextword1.h5\n",
      "61/61 [==============================] - 25s 410ms/step - loss: 2.3120 - lr: 0.0010\n",
      "Epoch 43/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.2557\n",
      "Epoch 43: loss improved from 2.31203 to 2.25568, saving model to nextword1.h5\n",
      "61/61 [==============================] - 25s 413ms/step - loss: 2.2557 - lr: 0.0010\n",
      "Epoch 44/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.1990\n",
      "Epoch 44: loss improved from 2.25568 to 2.19902, saving model to nextword1.h5\n",
      "61/61 [==============================] - 28s 461ms/step - loss: 2.1990 - lr: 0.0010\n",
      "Epoch 45/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.1426\n",
      "Epoch 45: loss improved from 2.19902 to 2.14259, saving model to nextword1.h5\n",
      "61/61 [==============================] - 27s 450ms/step - loss: 2.1426 - lr: 0.0010\n",
      "Epoch 46/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.1155\n",
      "Epoch 46: loss improved from 2.14259 to 2.11547, saving model to nextword1.h5\n",
      "61/61 [==============================] - 25s 403ms/step - loss: 2.1155 - lr: 0.0010\n",
      "Epoch 47/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.0882\n",
      "Epoch 47: loss improved from 2.11547 to 2.08825, saving model to nextword1.h5\n",
      "61/61 [==============================] - 25s 419ms/step - loss: 2.0882 - lr: 0.0010\n",
      "Epoch 48/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.0409\n",
      "Epoch 48: loss improved from 2.08825 to 2.04094, saving model to nextword1.h5\n",
      "61/61 [==============================] - 27s 448ms/step - loss: 2.0409 - lr: 0.0010\n",
      "Epoch 49/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.0174\n",
      "Epoch 49: loss improved from 2.04094 to 2.01742, saving model to nextword1.h5\n",
      "61/61 [==============================] - 27s 433ms/step - loss: 2.0174 - lr: 0.0010\n",
      "Epoch 50/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.9789\n",
      "Epoch 50: loss improved from 2.01742 to 1.97890, saving model to nextword1.h5\n",
      "61/61 [==============================] - 27s 446ms/step - loss: 1.9789 - lr: 0.0010\n",
      "Epoch 51/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.9504\n",
      "Epoch 51: loss improved from 1.97890 to 1.95036, saving model to nextword1.h5\n",
      "61/61 [==============================] - 27s 437ms/step - loss: 1.9504 - lr: 0.0010\n",
      "Epoch 52/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.9028\n",
      "Epoch 52: loss improved from 1.95036 to 1.90277, saving model to nextword1.h5\n",
      "61/61 [==============================] - 26s 436ms/step - loss: 1.9028 - lr: 0.0010\n",
      "Epoch 53/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.9030\n",
      "Epoch 53: loss did not improve from 1.90277\n",
      "61/61 [==============================] - 26s 429ms/step - loss: 1.9030 - lr: 0.0010\n",
      "Epoch 54/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.8578\n",
      "Epoch 54: loss improved from 1.90277 to 1.85785, saving model to nextword1.h5\n",
      "61/61 [==============================] - 26s 425ms/step - loss: 1.8578 - lr: 0.0010\n",
      "Epoch 55/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.8352\n",
      "Epoch 55: loss improved from 1.85785 to 1.83518, saving model to nextword1.h5\n",
      "61/61 [==============================] - 27s 443ms/step - loss: 1.8352 - lr: 0.0010\n",
      "Epoch 56/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.8132\n",
      "Epoch 56: loss improved from 1.83518 to 1.81319, saving model to nextword1.h5\n",
      "61/61 [==============================] - 26s 419ms/step - loss: 1.8132 - lr: 0.0010\n",
      "Epoch 57/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7989\n",
      "Epoch 57: loss improved from 1.81319 to 1.79891, saving model to nextword1.h5\n",
      "61/61 [==============================] - 25s 414ms/step - loss: 1.7989 - lr: 0.0010\n",
      "Epoch 58/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7523\n",
      "Epoch 58: loss improved from 1.79891 to 1.75235, saving model to nextword1.h5\n",
      "61/61 [==============================] - 25s 408ms/step - loss: 1.7523 - lr: 0.0010\n",
      "Epoch 59/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7250\n",
      "Epoch 59: loss improved from 1.75235 to 1.72498, saving model to nextword1.h5\n",
      "61/61 [==============================] - 25s 411ms/step - loss: 1.7250 - lr: 0.0010\n",
      "Epoch 60/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6691\n",
      "Epoch 60: loss improved from 1.72498 to 1.66911, saving model to nextword1.h5\n",
      "61/61 [==============================] - 25s 415ms/step - loss: 1.6691 - lr: 0.0010\n",
      "Epoch 61/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6406\n",
      "Epoch 61: loss improved from 1.66911 to 1.64059, saving model to nextword1.h5\n",
      "61/61 [==============================] - 30s 497ms/step - loss: 1.6406 - lr: 0.0010\n",
      "Epoch 62/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6417\n",
      "Epoch 62: loss did not improve from 1.64059\n",
      "61/61 [==============================] - 33s 536ms/step - loss: 1.6417 - lr: 0.0010\n",
      "Epoch 63/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6130\n",
      "Epoch 63: loss improved from 1.64059 to 1.61300, saving model to nextword1.h5\n",
      "61/61 [==============================] - 34s 554ms/step - loss: 1.6130 - lr: 0.0010\n",
      "Epoch 64/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5791\n",
      "Epoch 64: loss improved from 1.61300 to 1.57908, saving model to nextword1.h5\n",
      "61/61 [==============================] - 35s 569ms/step - loss: 1.5791 - lr: 0.0010\n",
      "Epoch 65/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5439\n",
      "Epoch 65: loss improved from 1.57908 to 1.54390, saving model to nextword1.h5\n",
      "61/61 [==============================] - 34s 568ms/step - loss: 1.5439 - lr: 0.0010\n",
      "Epoch 66/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5315\n",
      "Epoch 66: loss improved from 1.54390 to 1.53145, saving model to nextword1.h5\n",
      "61/61 [==============================] - 41s 663ms/step - loss: 1.5315 - lr: 0.0010\n",
      "Epoch 67/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5249\n",
      "Epoch 67: loss improved from 1.53145 to 1.52488, saving model to nextword1.h5\n",
      "61/61 [==============================] - 43s 713ms/step - loss: 1.5249 - lr: 0.0010\n",
      "Epoch 68/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5211\n",
      "Epoch 68: loss improved from 1.52488 to 1.52107, saving model to nextword1.h5\n",
      "61/61 [==============================] - 33s 549ms/step - loss: 1.5211 - lr: 0.0010\n",
      "Epoch 69/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5112\n",
      "Epoch 69: loss improved from 1.52107 to 1.51124, saving model to nextword1.h5\n",
      "61/61 [==============================] - 34s 553ms/step - loss: 1.5112 - lr: 0.0010\n",
      "Epoch 70/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4918\n",
      "Epoch 70: loss improved from 1.51124 to 1.49182, saving model to nextword1.h5\n",
      "61/61 [==============================] - 28s 450ms/step - loss: 1.4918 - lr: 0.0010\n",
      "Epoch 71/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4542\n",
      "Epoch 71: loss improved from 1.49182 to 1.45421, saving model to nextword1.h5\n",
      "61/61 [==============================] - 26s 421ms/step - loss: 1.4542 - lr: 0.0010\n",
      "Epoch 72/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4241\n",
      "Epoch 72: loss improved from 1.45421 to 1.42415, saving model to nextword1.h5\n",
      "61/61 [==============================] - 26s 431ms/step - loss: 1.4241 - lr: 0.0010\n",
      "Epoch 73/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4073\n",
      "Epoch 73: loss improved from 1.42415 to 1.40726, saving model to nextword1.h5\n",
      "61/61 [==============================] - 27s 438ms/step - loss: 1.4073 - lr: 0.0010\n",
      "Epoch 74/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3947\n",
      "Epoch 74: loss improved from 1.40726 to 1.39473, saving model to nextword1.h5\n",
      "61/61 [==============================] - 26s 432ms/step - loss: 1.3947 - lr: 0.0010\n",
      "Epoch 75/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3581\n",
      "Epoch 75: loss improved from 1.39473 to 1.35811, saving model to nextword1.h5\n",
      "61/61 [==============================] - 28s 458ms/step - loss: 1.3581 - lr: 0.0010\n",
      "Epoch 76/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3527\n",
      "Epoch 76: loss improved from 1.35811 to 1.35266, saving model to nextword1.h5\n",
      "61/61 [==============================] - 28s 454ms/step - loss: 1.3527 - lr: 0.0010\n",
      "Epoch 77/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3185\n",
      "Epoch 77: loss improved from 1.35266 to 1.31846, saving model to nextword1.h5\n",
      "61/61 [==============================] - 27s 451ms/step - loss: 1.3185 - lr: 0.0010\n",
      "Epoch 78/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3082\n",
      "Epoch 78: loss improved from 1.31846 to 1.30820, saving model to nextword1.h5\n",
      "61/61 [==============================] - 27s 446ms/step - loss: 1.3082 - lr: 0.0010\n",
      "Epoch 79/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3226\n",
      "Epoch 79: loss did not improve from 1.30820\n",
      "61/61 [==============================] - 26s 427ms/step - loss: 1.3226 - lr: 0.0010\n",
      "Epoch 80/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3046\n",
      "Epoch 80: loss improved from 1.30820 to 1.30461, saving model to nextword1.h5\n",
      "61/61 [==============================] - 27s 454ms/step - loss: 1.3046 - lr: 0.0010\n",
      "Epoch 81/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3034\n",
      "Epoch 81: loss improved from 1.30461 to 1.30341, saving model to nextword1.h5\n",
      "61/61 [==============================] - 28s 458ms/step - loss: 1.3034 - lr: 0.0010\n",
      "Epoch 82/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2869\n",
      "Epoch 82: loss improved from 1.30341 to 1.28694, saving model to nextword1.h5\n",
      "61/61 [==============================] - 26s 416ms/step - loss: 1.2869 - lr: 0.0010\n",
      "Epoch 83/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2678\n",
      "Epoch 83: loss improved from 1.28694 to 1.26781, saving model to nextword1.h5\n",
      "61/61 [==============================] - 29s 479ms/step - loss: 1.2678 - lr: 0.0010\n",
      "Epoch 84/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2366\n",
      "Epoch 84: loss improved from 1.26781 to 1.23655, saving model to nextword1.h5\n",
      "61/61 [==============================] - 27s 443ms/step - loss: 1.2366 - lr: 0.0010\n",
      "Epoch 85/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2408\n",
      "Epoch 85: loss did not improve from 1.23655\n",
      "61/61 [==============================] - 25s 416ms/step - loss: 1.2408 - lr: 0.0010\n",
      "Epoch 86/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2146\n",
      "Epoch 86: loss improved from 1.23655 to 1.21459, saving model to nextword1.h5\n",
      "61/61 [==============================] - 27s 454ms/step - loss: 1.2146 - lr: 0.0010\n",
      "Epoch 87/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1804\n",
      "Epoch 87: loss improved from 1.21459 to 1.18040, saving model to nextword1.h5\n",
      "61/61 [==============================] - 29s 469ms/step - loss: 1.1804 - lr: 0.0010\n",
      "Epoch 88/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1834\n",
      "Epoch 88: loss did not improve from 1.18040\n",
      "61/61 [==============================] - 27s 435ms/step - loss: 1.1834 - lr: 0.0010\n",
      "Epoch 89/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1786\n",
      "Epoch 89: loss improved from 1.18040 to 1.17864, saving model to nextword1.h5\n",
      "61/61 [==============================] - 27s 446ms/step - loss: 1.1786 - lr: 0.0010\n",
      "Epoch 90/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1453\n",
      "Epoch 90: loss improved from 1.17864 to 1.14533, saving model to nextword1.h5\n",
      "61/61 [==============================] - 28s 455ms/step - loss: 1.1453 - lr: 0.0010\n",
      "Epoch 91/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1403\n",
      "Epoch 91: loss improved from 1.14533 to 1.14034, saving model to nextword1.h5\n",
      "61/61 [==============================] - 28s 459ms/step - loss: 1.1403 - lr: 0.0010\n",
      "Epoch 92/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1135\n",
      "Epoch 92: loss improved from 1.14034 to 1.11347, saving model to nextword1.h5\n",
      "61/61 [==============================] - 29s 481ms/step - loss: 1.1135 - lr: 0.0010\n",
      "Epoch 93/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1008\n",
      "Epoch 93: loss improved from 1.11347 to 1.10083, saving model to nextword1.h5\n",
      "61/61 [==============================] - 29s 477ms/step - loss: 1.1008 - lr: 0.0010\n",
      "Epoch 94/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0958\n",
      "Epoch 94: loss improved from 1.10083 to 1.09578, saving model to nextword1.h5\n",
      "61/61 [==============================] - 29s 471ms/step - loss: 1.0958 - lr: 0.0010\n",
      "Epoch 95/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0660\n",
      "Epoch 95: loss improved from 1.09578 to 1.06596, saving model to nextword1.h5\n",
      "61/61 [==============================] - 30s 493ms/step - loss: 1.0660 - lr: 0.0010\n",
      "Epoch 96/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0510\n",
      "Epoch 96: loss improved from 1.06596 to 1.05102, saving model to nextword1.h5\n",
      "61/61 [==============================] - 30s 494ms/step - loss: 1.0510 - lr: 0.0010\n",
      "Epoch 97/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0397\n",
      "Epoch 97: loss improved from 1.05102 to 1.03975, saving model to nextword1.h5\n",
      "61/61 [==============================] - 32s 526ms/step - loss: 1.0397 - lr: 0.0010\n",
      "Epoch 98/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0295\n",
      "Epoch 98: loss improved from 1.03975 to 1.02955, saving model to nextword1.h5\n",
      "61/61 [==============================] - 28s 455ms/step - loss: 1.0295 - lr: 0.0010\n",
      "Epoch 99/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0115\n",
      "Epoch 99: loss improved from 1.02955 to 1.01152, saving model to nextword1.h5\n",
      "61/61 [==============================] - 30s 487ms/step - loss: 1.0115 - lr: 0.0010\n",
      "Epoch 100/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0123\n",
      "Epoch 100: loss did not improve from 1.01152\n",
      "61/61 [==============================] - 30s 479ms/step - loss: 1.0123 - lr: 0.0010\n",
      "Epoch 101/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0007\n",
      "Epoch 101: loss improved from 1.01152 to 1.00074, saving model to nextword1.h5\n",
      "61/61 [==============================] - 31s 504ms/step - loss: 1.0007 - lr: 0.0010\n",
      "Epoch 102/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0000\n",
      "Epoch 102: loss improved from 1.00074 to 0.99996, saving model to nextword1.h5\n",
      "61/61 [==============================] - 32s 522ms/step - loss: 1.0000 - lr: 0.0010\n",
      "Epoch 103/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0025\n",
      "Epoch 103: loss did not improve from 0.99996\n",
      "61/61 [==============================] - 29s 464ms/step - loss: 1.0025 - lr: 0.0010\n",
      "Epoch 104/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9872\n",
      "Epoch 104: loss improved from 0.99996 to 0.98717, saving model to nextword1.h5\n",
      "61/61 [==============================] - 28s 460ms/step - loss: 0.9872 - lr: 0.0010\n",
      "Epoch 105/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.9911\n",
      "Epoch 105: loss did not improve from 0.98717\n",
      "61/61 [==============================] - 36s 593ms/step - loss: 0.9911 - lr: 0.0010\n",
      "Epoch 106/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0103\n",
      "Epoch 106: loss did not improve from 0.98717\n",
      "61/61 [==============================] - 31s 516ms/step - loss: 1.0103 - lr: 0.0010\n",
      "Epoch 107/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0244\n",
      "Epoch 107: loss did not improve from 0.98717\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "61/61 [==============================] - 33s 549ms/step - loss: 1.0244 - lr: 0.0010\n",
      "Epoch 108/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7848\n",
      "Epoch 108: loss improved from 0.98717 to 0.78478, saving model to nextword1.h5\n",
      "61/61 [==============================] - 32s 536ms/step - loss: 0.7848 - lr: 2.0000e-04\n",
      "Epoch 109/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.7129\n",
      "Epoch 109: loss improved from 0.78478 to 0.71290, saving model to nextword1.h5\n",
      "61/61 [==============================] - 31s 503ms/step - loss: 0.7129 - lr: 2.0000e-04\n",
      "Epoch 110/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6919\n",
      "Epoch 110: loss improved from 0.71290 to 0.69190, saving model to nextword1.h5\n",
      "61/61 [==============================] - 32s 527ms/step - loss: 0.6919 - lr: 2.0000e-04\n",
      "Epoch 111/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6816\n",
      "Epoch 111: loss improved from 0.69190 to 0.68159, saving model to nextword1.h5\n",
      "61/61 [==============================] - 30s 491ms/step - loss: 0.6816 - lr: 2.0000e-04\n",
      "Epoch 112/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6754\n",
      "Epoch 112: loss improved from 0.68159 to 0.67538, saving model to nextword1.h5\n",
      "61/61 [==============================] - 28s 459ms/step - loss: 0.6754 - lr: 2.0000e-04\n",
      "Epoch 113/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6729\n",
      "Epoch 113: loss improved from 0.67538 to 0.67287, saving model to nextword1.h5\n",
      "61/61 [==============================] - 31s 506ms/step - loss: 0.6729 - lr: 2.0000e-04\n",
      "Epoch 114/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6703\n",
      "Epoch 114: loss improved from 0.67287 to 0.67030, saving model to nextword1.h5\n",
      "61/61 [==============================] - 30s 497ms/step - loss: 0.6703 - lr: 2.0000e-04\n",
      "Epoch 115/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6692\n",
      "Epoch 115: loss improved from 0.67030 to 0.66918, saving model to nextword1.h5\n",
      "61/61 [==============================] - 34s 562ms/step - loss: 0.6692 - lr: 2.0000e-04\n",
      "Epoch 116/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6684\n",
      "Epoch 116: loss improved from 0.66918 to 0.66841, saving model to nextword1.h5\n",
      "61/61 [==============================] - 31s 519ms/step - loss: 0.6684 - lr: 2.0000e-04\n",
      "Epoch 117/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6679\n",
      "Epoch 117: loss improved from 0.66841 to 0.66793, saving model to nextword1.h5\n",
      "61/61 [==============================] - 30s 490ms/step - loss: 0.6679 - lr: 2.0000e-04\n",
      "Epoch 118/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6681\n",
      "Epoch 118: loss did not improve from 0.66793\n",
      "61/61 [==============================] - 27s 443ms/step - loss: 0.6681 - lr: 2.0000e-04\n",
      "Epoch 119/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6666\n",
      "Epoch 119: loss improved from 0.66793 to 0.66661, saving model to nextword1.h5\n",
      "61/61 [==============================] - 30s 493ms/step - loss: 0.6666 - lr: 2.0000e-04\n",
      "Epoch 120/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6679\n",
      "Epoch 120: loss did not improve from 0.66661\n",
      "61/61 [==============================] - 30s 490ms/step - loss: 0.6679 - lr: 2.0000e-04\n",
      "Epoch 121/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6667\n",
      "Epoch 121: loss did not improve from 0.66661\n",
      "61/61 [==============================] - 29s 478ms/step - loss: 0.6667 - lr: 2.0000e-04\n",
      "Epoch 122/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6655\n",
      "Epoch 122: loss improved from 0.66661 to 0.66552, saving model to nextword1.h5\n",
      "61/61 [==============================] - 29s 471ms/step - loss: 0.6655 - lr: 2.0000e-04\n",
      "Epoch 123/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6655\n",
      "Epoch 123: loss did not improve from 0.66552\n",
      "61/61 [==============================] - 30s 481ms/step - loss: 0.6655 - lr: 2.0000e-04\n",
      "Epoch 124/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6663\n",
      "Epoch 124: loss did not improve from 0.66552\n",
      "61/61 [==============================] - 30s 491ms/step - loss: 0.6663 - lr: 2.0000e-04\n",
      "Epoch 125/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6655\n",
      "Epoch 125: loss improved from 0.66552 to 0.66550, saving model to nextword1.h5\n",
      "\n",
      "Epoch 125: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "61/61 [==============================] - 30s 494ms/step - loss: 0.6655 - lr: 2.0000e-04\n",
      "Epoch 126/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6284\n",
      "Epoch 126: loss improved from 0.66550 to 0.62839, saving model to nextword1.h5\n",
      "61/61 [==============================] - 29s 460ms/step - loss: 0.6284 - lr: 1.0000e-04\n",
      "Epoch 127/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6267\n",
      "Epoch 127: loss improved from 0.62839 to 0.62675, saving model to nextword1.h5\n",
      "61/61 [==============================] - 33s 539ms/step - loss: 0.6267 - lr: 1.0000e-04\n",
      "Epoch 128/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6259\n",
      "Epoch 128: loss improved from 0.62675 to 0.62587, saving model to nextword1.h5\n",
      "61/61 [==============================] - 31s 507ms/step - loss: 0.6259 - lr: 1.0000e-04\n",
      "Epoch 129/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6254\n",
      "Epoch 129: loss improved from 0.62587 to 0.62545, saving model to nextword1.h5\n",
      "61/61 [==============================] - 31s 511ms/step - loss: 0.6254 - lr: 1.0000e-04\n",
      "Epoch 130/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6252\n",
      "Epoch 130: loss improved from 0.62545 to 0.62522, saving model to nextword1.h5\n",
      "61/61 [==============================] - 31s 511ms/step - loss: 0.6252 - lr: 1.0000e-04\n",
      "Epoch 131/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6246\n",
      "Epoch 131: loss improved from 0.62522 to 0.62458, saving model to nextword1.h5\n",
      "61/61 [==============================] - 31s 507ms/step - loss: 0.6246 - lr: 1.0000e-04\n",
      "Epoch 132/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6251\n",
      "Epoch 132: loss did not improve from 0.62458\n",
      "61/61 [==============================] - 30s 502ms/step - loss: 0.6251 - lr: 1.0000e-04\n",
      "Epoch 133/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6238\n",
      "Epoch 133: loss improved from 0.62458 to 0.62377, saving model to nextword1.h5\n",
      "61/61 [==============================] - 30s 494ms/step - loss: 0.6238 - lr: 1.0000e-04\n",
      "Epoch 134/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6241\n",
      "Epoch 134: loss did not improve from 0.62377\n",
      "61/61 [==============================] - 31s 498ms/step - loss: 0.6241 - lr: 1.0000e-04\n",
      "Epoch 135/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6250\n",
      "Epoch 135: loss did not improve from 0.62377\n",
      "61/61 [==============================] - 34s 552ms/step - loss: 0.6250 - lr: 1.0000e-04\n",
      "Epoch 136/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6239\n",
      "Epoch 136: loss did not improve from 0.62377\n",
      "61/61 [==============================] - 34s 553ms/step - loss: 0.6239 - lr: 1.0000e-04\n",
      "Epoch 137/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6245\n",
      "Epoch 137: loss did not improve from 0.62377\n",
      "61/61 [==============================] - 32s 520ms/step - loss: 0.6245 - lr: 1.0000e-04\n",
      "Epoch 138/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6239\n",
      "Epoch 138: loss did not improve from 0.62377\n",
      "61/61 [==============================] - 34s 549ms/step - loss: 0.6239 - lr: 1.0000e-04\n",
      "Epoch 139/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6234\n",
      "Epoch 139: loss improved from 0.62377 to 0.62339, saving model to nextword1.h5\n",
      "61/61 [==============================] - 31s 513ms/step - loss: 0.6234 - lr: 1.0000e-04\n",
      "Epoch 140/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6243\n",
      "Epoch 140: loss did not improve from 0.62339\n",
      "61/61 [==============================] - 28s 464ms/step - loss: 0.6243 - lr: 1.0000e-04\n",
      "Epoch 141/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6228\n",
      "Epoch 141: loss improved from 0.62339 to 0.62280, saving model to nextword1.h5\n",
      "61/61 [==============================] - 31s 501ms/step - loss: 0.6228 - lr: 1.0000e-04\n",
      "Epoch 142/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6233\n",
      "Epoch 142: loss did not improve from 0.62280\n",
      "61/61 [==============================] - 34s 563ms/step - loss: 0.6233 - lr: 1.0000e-04\n",
      "Epoch 143/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6225\n",
      "Epoch 143: loss improved from 0.62280 to 0.62254, saving model to nextword1.h5\n",
      "61/61 [==============================] - 30s 499ms/step - loss: 0.6225 - lr: 1.0000e-04\n",
      "Epoch 144/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6234\n",
      "Epoch 144: loss did not improve from 0.62254\n",
      "61/61 [==============================] - 32s 522ms/step - loss: 0.6234 - lr: 1.0000e-04\n",
      "Epoch 145/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6227\n",
      "Epoch 145: loss did not improve from 0.62254\n",
      "61/61 [==============================] - 33s 544ms/step - loss: 0.6227 - lr: 1.0000e-04\n",
      "Epoch 146/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6227\n",
      "Epoch 146: loss did not improve from 0.62254\n",
      "61/61 [==============================] - 33s 542ms/step - loss: 0.6227 - lr: 1.0000e-04\n",
      "Epoch 147/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6228\n",
      "Epoch 147: loss did not improve from 0.62254\n",
      "61/61 [==============================] - 32s 520ms/step - loss: 0.6228 - lr: 1.0000e-04\n",
      "Epoch 148/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6224\n",
      "Epoch 148: loss improved from 0.62254 to 0.62240, saving model to nextword1.h5\n",
      "61/61 [==============================] - 30s 501ms/step - loss: 0.6224 - lr: 1.0000e-04\n",
      "Epoch 149/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6225\n",
      "Epoch 149: loss did not improve from 0.62240\n",
      "61/61 [==============================] - 32s 528ms/step - loss: 0.6225 - lr: 1.0000e-04\n",
      "Epoch 150/150\n",
      "61/61 [==============================] - ETA: 0s - loss: 0.6219\n",
      "Epoch 150: loss improved from 0.62240 to 0.62187, saving model to nextword1.h5\n",
      "61/61 [==============================] - 31s 502ms/step - loss: 0.6219 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14256949a30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/26649716/how-to-show-pil-image-in-ipython-notebook\n",
    "# tensorboard --logdir=\"./logsnextword1\"\n",
    "# http://DESKTOP-U3TSCVT:6006/\n",
    "\n",
    "# from IPython.display import Image \n",
    "# pil_img = Image(filename='graph1.png')\n",
    "# display(pil_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation:\n",
    "### We are able to develop a decent next word prediction model and are able to get a declining loss and an overall decent performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mpredict_classes([\u001b[39m761\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "model.predict_classes([761])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
